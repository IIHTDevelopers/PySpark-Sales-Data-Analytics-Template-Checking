{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644bb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f242ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate spark session\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"appName\").config(\"spark.driver.extraClassPath\", \"C:\\\\mysql-connector-j-8.0.33.jar\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d7bdc79",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e53b9aa2",
   "metadata": {},
   "source": [
    "## Q1) 1.1 Load data from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a9b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_mysql(spark :pyspark.sql.SparkSession, db_name :str, table_name :str) \\\n",
    "                        -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "        â€“ load data from MySQL table 'table_name' from database 'db_name' and return the \n",
    "        table as a spark dataframe.\n",
    "\n",
    "        For the mysql connection use below information:\n",
    "            jdbc driver: 'com.mysql.cj.jdbc.Driver'\n",
    "            Hostname: 'localhost'\n",
    "            Port: 3306\n",
    "            Database: 'classicmodels'\n",
    "            Table_name: 'order_details'\n",
    "            Username: 'root'\n",
    "            Password: 'pass@word1'\n",
    "    '''\n",
    "    # your code goes here\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86a88f4a",
   "metadata": {},
   "source": [
    "## Q2) 1.2 Load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d548d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(spark, csv_file_name):\n",
    "        '''\n",
    "        Load data from CSV file 'csv_file_name' and return a spark Dataframe\n",
    "        '''\n",
    "        # your code goes here\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c810a4aa",
   "metadata": {},
   "source": [
    "## Q3) 1.3  Load data from flat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_flatfile(spark :pyspark.sql.SparkSession, txt_file_name: str) \\\n",
    "                      -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Load data from flat file 'txt_file_name' separated with ':' and return a spark Dataframe\n",
    "    PS: The data files for this assignment are in 'data' folder\n",
    "    You can access full path of 'data/' folder using 'DATA_FOLDER' variable\n",
    "    from constants.py\n",
    "    '''\n",
    "    # your code goes here\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b29688d",
   "metadata": {},
   "source": [
    "# 2. Transformations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72a25702",
   "metadata": {},
   "source": [
    "## Q4) 2.1  Clean product's MSRP data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524eb91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_product_MSRP_column(spark :pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Due to a data entry issues MSRP, the selling price is lower than its buyPrice for some\n",
    "    products. Change MSRP to 1.4 times of the buyPrice for such products and cast it to two\n",
    "    decimal places.\n",
    "    \n",
    "    Return a spark dataframe with following columns.\n",
    "    |productCode|productName|productLine|productScale|productVendor|productDescription|quantityInStock|buyPrice|MSRP|\n",
    "    '''\n",
    "    # your code goes here\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71c4a4da",
   "metadata": {},
   "source": [
    "## Q5) 2.2  Apply explode on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b4a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_productLines_description(spark :pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    ''' \n",
    "    Split the productLines description into words and explode the it.\n",
    "\n",
    "    Return a spark dataframe with following columns.\n",
    "    |productLine|description|\n",
    "    +-----------+------------+\n",
    "    |Motorcycles|         Our|\n",
    "    |Motorcycles| motorcycles|\n",
    "    |Motorcycles|         are|\n",
    "       ... so on\n",
    "\n",
    "    '''\n",
    "    # your code goes here\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dae8471",
   "metadata": {},
   "source": [
    "## Q6) 2.3  Struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e2865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_info(spark :pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Return a consolidated customer info using structs.\n",
    "    \n",
    "    Return a spark dataframe with following columns.\n",
    "    |custID|custName|country|#orders|totalMoneySpent|creditLimit|\n",
    "    '''\n",
    "    customer_schema = StructType([ \\\n",
    "      StructField(\"custID\",StringType(),True), \\\n",
    "      StructField(\"custName\",StringType(),True), \\\n",
    "      StructField(\"country\",StringType(),True), \\\n",
    "      StructField(\"#orders\",IntegerType(),True), \\\n",
    "      StructField(\"totalMoneySpent\", DoubleType(), True), \\\n",
    "      StructField(\"creditLimit\", IntegerType(), True), \\\n",
    "    ])\n",
    "    # your code goes here\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f69b4082",
   "metadata": {},
   "source": [
    "## Q7) 2.4  UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc833f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def categorize_products(buy_price):\n",
    "    if buy_price < 40:\n",
    "        return \"Affordable\"\n",
    "    elif buy_price < 80:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"Expensive\"\n",
    "    \n",
    "def retrun_product_category(spark :pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    ''' \n",
    "    Add a new column 'productCategory' using above udf and return the updated dataframe\n",
    "    Note: Please do not change original dataframe.\n",
    "\n",
    "    Return a spark dataframe with following columns.\n",
    "    |productCode|productName|productLine|productScale|productVendor|productDescription \\\n",
    "    |quantityInStock|buyPrice|MSRP|productCategory|\n",
    "    '''\n",
    "    # your code goes here\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da0469f0",
   "metadata": {},
   "source": [
    "## Q8) 2.5  Rotate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e352150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_orders_by_day_by_status(spark :pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Return No. of orders by weekday(1 to 7) by order status.\n",
    "\n",
    "    Return a spark dataframe with following columns.\n",
    "    +-------+---------+--------+----------+-------+--------+-------+\n",
    "    |weekDay|Cancelled|Disputed|In Process|On Hold|Resolved|Shipped|\n",
    "    +-------+---------+--------+----------+-------+--------+-------+\n",
    "    |      1|     null|    null|         2|      1|    null|      8|\n",
    "           2\n",
    "           3 and so on\n",
    "    '''\n",
    "    # your code goes here\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "894f55de",
   "metadata": {},
   "source": [
    "## Q9) 2.6  Clean ProductLines text Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_productlines_description(spark :pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Take testDescription from productLines table and clean it.\n",
    "    1. Convert to lower-case\n",
    "    2. Remove all special characters except a-z and 0-9.\n",
    "    3. Remove stop words (using StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "    The final value(testDescription) must be a string.\n",
    "    \n",
    "    Return a spark dataframe with following columns.\n",
    "    |productLine|htmlDescription|image|textDescription|\n",
    "    '''\n",
    "    # your code goes here\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d72f2ab",
   "metadata": {},
   "source": [
    "# 3. Analytics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f24cae3c",
   "metadata": {},
   "source": [
    "## Q10) 3.1 Your company wants to promote their top 5 employees who made maximum sales(orders)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d5dbfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_5_employees(spark :pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Return the top 5 employees(Sales Representatives) who made maximum no. of\n",
    "    sales throughout the given timeperiod. You can consider all sales with all statuses.\n",
    "    \n",
    "    Return a spark dataframe with following columns.\n",
    "    employeeNumber|firstName|lastName |TotalOrders\n",
    "    '''\n",
    "    # your code goes here\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cb039cf",
   "metadata": {},
   "source": [
    "## Q11) 3.2 Imagine you are a sales representative. Mail your manager a report on cancelled orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a8e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_cancelled_orders(spark :pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Return email adresses of your manager to send a report about the cancelled orders. Return\n",
    "    Order details and a column to define if the cancelled order has been shipped already\n",
    "    or not('Yes' or  'No').\n",
    "     \n",
    "    Return a spark dataframe with following columns.\n",
    "    +-----------+---------+--------------------+--------------+----------------------+---------+--------------------+\n",
    "    |orderNumber|isShipped|            comments|customerNumber|salesRepEmployeeNumber|reportsTo|               email|\n",
    "    +-----------+---------+--------------------+--------------+----------------------+---------+--------------------+\n",
    "    |      10167|       No|Customer called t...|           448|                  1504|     1102|gbondur@classicmo...|\n",
    "           ...\n",
    "           ... so on\n",
    "    '''\n",
    "    # your code goes here\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a027cf1",
   "metadata": {},
   "source": [
    "## Q12) 3.3 Return Top 5 big spenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0c8be966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_5_big_spenders(spark :pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Return top 5 big spenders who had spent the most $(highest order value).\n",
    "    \n",
    "    Return a spark dataframe with following columns.\n",
    "    customerNumber|customerName|totalOrderValue|\n",
    "    '''\n",
    "    # your code goes here\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d807e972",
   "metadata": {},
   "source": [
    "## Q13) 3.4 Return Top 5 big spenders(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457667a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_top_5_big_spend_countries(spark :pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Return top 5 big countries which had spent the most $(highest order value).\n",
    "    \n",
    "    Return a spark dataframe with following columns.\n",
    "    |country|totalOrderValue|\n",
    "    '''\n",
    "    # your code goes here\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
